steps:
  - label: "Simple Unit Test"
    depends_on: upload-ready-pipeline
    commands:
      - "timeout 20m pytest -v -s -m 'core_model and cpu' --cov=vllm_omni --cov-branch --cov-report=term-missing --cov-report=html --cov-report=xml"
    agents:
      queue: "gpu_1_queue"
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          environment:
            - "HF_HOME=/fsx/hf_cache"
          volumes:
            - "/fsx/hf_cache:/fsx/hf_cache"

  - label: "Diffusion Model Test"
    depends_on: upload-ready-pipeline
    commands:
      - timeout 20m pytest -s -v tests/e2e/offline_inference/test_t2i_model.py -m "core_model and diffusion" --run-level "core_model"
    agents:
      queue: "gpu_1_queue"
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          environment:
            - "HF_HOME=/fsx/hf_cache"
          volumes:
            - "/fsx/hf_cache:/fsx/hf_cache"

  - label: "Diffusion Model CPU offloading Test"
    depends_on: upload-ready-pipeline
    commands:
      - |
        timeout 20m bash -c '
          set +e
          pytest -s -v tests/e2e/offline_inference/test_diffusion_cpu_offload.py
          EXIT1=$$?
          pytest -s -v tests/e2e/offline_inference/test_diffusion_layerwise_offload.py
          EXIT2=$$?
          exit $$((EXIT1 | EXIT2))
        '
    agents:
      queue: "gpu_1_queue" # g6.4xlarge instance on AWS, has 1 L4 GPU
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          environment:
            - "HF_HOME=/fsx/hf_cache"
          volumes:
            - "/fsx/hf_cache:/fsx/hf_cache"

  - label: "Audio Generation Model Test"
    depends_on: upload-ready-pipeline
    commands:
      - timeout 20m pytest -s -v tests/e2e/offline_inference/test_stable_audio_model.py
    agents:
      queue: "gpu_1_queue" # g6.4xlarge instance on AWS, has 1 L4 GPU
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          environment:
            - "HF_HOME=/fsx/hf_cache"
          volumes:
            - "/fsx/hf_cache:/fsx/hf_cache"

  - label: "Diffusion Cache Backend Test"
    depends_on: upload-ready-pipeline
    commands:
      - timeout 15m pytest -s -v -m 'core_model and cache and diffusion and not distributed_cuda and L4'
    agents:
      queue: "gpu_1_queue" # g6.4xlarge instance on AWS, has 1 L4 GPU
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          environment:
            - "HF_HOME=/fsx/hf_cache"
          volumes:
            - "/fsx/hf_cache:/fsx/hf_cache"

  - label: "Diffusion Sequence Parallelism Test"
    depends_on: upload-ready-pipeline
    commands:
      - timeout 20m pytest -s -v tests/e2e/offline_inference/test_sequence_parallel.py -m core_model
    agents:
      queue: "gpu_4_queue" # g6.12xlarge instance on AWS, has 4 L4 GPU
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          shm-size: "8gb"
          environment:
            - "HF_HOME=/fsx/hf_cache"
          volumes:
            - "/fsx/hf_cache:/fsx/hf_cache"

  - label: "Diffusion GPU Worker Test"
    depends_on: upload-ready-pipeline
    commands:
      - timeout 20m pytest -s -v tests/diffusion/test_diffusion_worker.py
    agents:
      queue: "gpu_4_queue" # g6.12xlarge instance on AWS, has 4 L4 GPU
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          shm-size: "8gb"
          environment:
            - "HF_HOME=/fsx/hf_cache"
          volumes:
            - "/fsx/hf_cache:/fsx/hf_cache"


  - label: "Benchmark & Engine Test"
    depends_on: upload-ready-pipeline
    commands:
      - |
        timeout 15m bash -c '
                export VLLM_WORKER_MULTIPROC_METHOD=spawn
                set +e
                pytest -s -v tests/benchmarks/test_serve_cli.py
                EXIT1=$$?
                pytest -s -v tests/engine/test_async_omni_engine_abort.py
                EXIT2=$$?
                exit $$((EXIT1 | EXIT2))
        '
    agents:
      queue: "gpu_4_queue" # g6.12xlarge instance on AWS, has 4 L4 GPU
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          shm-size: "8gb"
          environment:
            - "HF_HOME=/fsx/hf_cache"
          volumes:
            - "/fsx/hf_cache:/fsx/hf_cache"


  - label: "Omni Model Test"
    depends_on: upload-ready-pipeline
    commands:
      - |
        timeout 17m bash -c '
          export VLLM_LOGGING_LEVEL=DEBUG
          export VLLM_WORKER_MULTIPROC_METHOD=spawn
          pytest -s -v tests/e2e/offline_inference/test_qwen2_5_omni.py
        '
    agents:
      queue: "gpu_4_queue" # g6.12xlarge instance on AWS, has 4 L4 GPU
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          environment:
            - "HF_HOME=/fsx/hf_cache"
          volumes:
            - "/fsx/hf_cache:/fsx/hf_cache"

  # - label: "Omni Model Test with H100"
  #   depends_on: upload-ready-pipeline
  #   commands:
  #     - |
  #       timeout 20m bash -c '
  #         export VLLM_WORKER_MULTIPROC_METHOD=spawn
  #         export VLLM_TEST_CLEAN_GPU_MEMORY="1"
  #         # - pytest -s -v tests/e2e/offline_inference/test_qwen3_omni.py
  #         pytest -s -v tests/e2e/online_serving/test_qwen3_omni.py -m "core_model" --run-level "core_model"
  #       '
  #   agents:
  #     queue: "mithril-h100-pool"
  #   plugins:
  #     - kubernetes:
  #         podSpec:
  #           containers:
  #             - image: 936637512419.dkr.ecr.us-west-2.amazonaws.com/vllm-ci-pull-through-cache/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
  #               resources:
  #                 limits:
  #                   nvidia.com/gpu: 2
  #               volumeMounts:
  #                 - name: devshm
  #                   mountPath: /dev/shm
  #                 - name: hf-cache
  #                   mountPath: /root/.cache/huggingface
  #               env:
  #                 - name: HF_HOME
  #                   value: /root/.cache/huggingface
  #           nodeSelector:
  #             node.kubernetes.io/instance-type: gpu-h100-sxm
  #           volumes:
  #             - name: devshm
  #               emptyDir:
  #                 medium: Memory
  #             - name: hf-cache
  #               hostPath:
  #                 path: /mnt/hf-cache
  #                 type: DirectoryOrCreate

  - label: "Qwen3-TTS E2E Test"
    depends_on: upload-ready-pipeline
    commands:
      - |
        timeout 20m bash -c '
          export VLLM_LOGGING_LEVEL=DEBUG
          export VLLM_WORKER_MULTIPROC_METHOD=spawn
          pytest -s -v tests/e2e/online_serving/test_qwen3_tts.py
        '
    agents:
      queue: "gpu_4_queue"
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          shm-size: "8gb"
          environment:
            - "HF_HOME=/fsx/hf_cache"
          volumes:
            - "/fsx/hf_cache:/fsx/hf_cache"

  # - label: "Diffusion Image Edit Test with H100 (1 GPU)"
  #   depends_on: upload-ready-pipeline
  #   commands:
  #     - |
  #       timeout 20m bash -c '
  #         export VLLM_WORKER_MULTIPROC_METHOD=spawn
  #         pytest -s -v tests/e2e/online_serving/test_image_gen_edit.py
  #       '
  #   agents:
  #     queue: "mithril-h100-pool"
  #   plugins:
  #     - kubernetes:
  #         podSpec:
  #           containers:
  #             - image: 936637512419.dkr.ecr.us-west-2.amazonaws.com/vllm-ci-pull-through-cache/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
  #               resources:
  #                 limits:
  #                   nvidia.com/gpu: 1
  #               volumeMounts:
  #                 - name: devshm
  #                   mountPath: /dev/shm
  #                 - name: hf-cache
  #                   mountPath: /root/.cache/huggingface
  #               env:
  #                 - name: HF_HOME
  #                   value: /root/.cache/huggingface
  #           nodeSelector:
  #             node.kubernetes.io/instance-type: gpu-h100-sxm
  #           volumes:
  #             - name: devshm
  #               emptyDir:
  #                 medium: Memory
  #             - name: hf-cache
  #               hostPath:
  #                 path: /mnt/hf-cache
  #                 type: DirectoryOrCreate

  # - label: "Bagel Text2Img Model Test with H100"
  #   depends_on: upload-ready-pipeline
  #   commands:
  #     - |
  #       timeout 30m bash -c '
  #         export VLLM_WORKER_MULTIPROC_METHOD=spawn
  #         pytest -s -v tests/e2e/offline_inference/test_bagel_text2img.py
  #       '
  #   agents:
  #     queue: "mithril-h100-pool"
  #   plugins:
  #     - kubernetes:
  #         podSpec:
  #           containers:
  #             - image: 936637512419.dkr.ecr.us-west-2.amazonaws.com/vllm-ci-pull-through-cache/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
  #               resources:
  #                 limits:
  #                   nvidia.com/gpu: 1
  #               volumeMounts:
  #                 - name: devshm
  #                   mountPath: /dev/shm
  #                 - name: hf-cache
  #                   mountPath: /root/.cache/huggingface
  #               env:
  #                 - name: HF_HOME
  #                   value: /root/.cache/huggingface
  #           nodeSelector:
  #             node.kubernetes.io/instance-type: gpu-h100-sxm
  #           volumes:
  #             - name: devshm
  #               emptyDir:
  #                 medium: Memory
  #             - name: hf-cache
  #               hostPath:
  #                 path: /mnt/hf-cache
  #                 type: DirectoryOrCreate
