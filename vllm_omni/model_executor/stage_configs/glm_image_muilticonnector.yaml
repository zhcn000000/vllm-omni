# Stage config for running GLM-Image with 2-stage architecture (MultiConnector version)
# Stage 0: AR Model (vLLM implementation) - generates prior_token_ids
# Stage 1: Diffusion (DiT + VAE) - denoising and image decoding
#
# This config uses OmniConnectors for inter-stage communication,
# enabling efficient tensor transfer between stages on different processes/nodes.

stage_args:
  # Stage 0: AR Model (GlmImageForConditionalGeneration)
  # This stage uses the vLLM-optimized AR model to generate prior tokens
  # for conditioning the diffusion process.
  - stage_id: 0
    stage_type: llm
    runtime:
      process: true
      devices: "0"
      max_batch_size: 1
      requires_multimodal_data: true # Required for i2i mode to receive source images
    engine_args:
      model_stage: ar
      model_arch: GlmImageForConditionalGeneration
      worker_cls: vllm_omni.worker.gpu_ar_worker.GPUARWorker
      scheduler_cls: vllm_omni.core.sched.omni_ar_scheduler.OmniARScheduler
      gpu_memory_utilization: 0.6
      enforce_eager: false
      trust_remote_code: true
      engine_output_type: token_ids # Output prior_token_ids for diffusion stage
      distributed_executor_backend: "mp"
      enable_prefix_caching: false
      max_num_batched_tokens: 32768
      hf_config_name: vision_language_encoder # Subfolder in model path
    final_output: false # AR is not the final output
    is_comprehension: true
    default_sampling_params:
      temperature: 0.9 # From model's generation_config.json
      top_p: 0.75 # From model's generation_config.json
      top_k: 16512 # vision_vocab_size from generation_config.json
      max_tokens: 1281 # For 1024x1024: small(16x16=256) + large(32x32=1024) + EOS(1)
      stop_token_ids: [16385] # eos_token_id from generation_config.json
      seed: 42
      detokenize: false

  # Stage 1: Diffusion (DiT + VAE)
  # This stage receives prior_token_ids from AR and performs denoising + VAE decode
  - stage_id: 1
    stage_type: diffusion
    runtime:
      process: true
      devices: "1" # Use separate GPU for diffusion
      max_batch_size: 1
      requires_multimodal_data: true # Required for i2i mode to pass condition images
    engine_args:
      model_stage: dit
      # Diffusion-specific parameters
      num_gpus: 1
      enforce_eager: true
      trust_remote_code: true
      distributed_executor_backend: "mp"
    engine_input_source: [0] # Input from AR stage
    custom_process_input_func: vllm_omni.model_executor.stage_input_processors.glm_image.ar2diffusion
    final_output: true
    final_output_type: image
    default_sampling_params:
      seed: 42
      num_inference_steps: 50
      guidance_scale: 1.5
      height: 1024
      width: 1024

# Top-level runtime config with MultiConnector support
runtime:
  enabled: true
  defaults:
    window_size: -1 # Trigger downstream only after full upstream completion
    max_inflight: 1 # Process serially within each stage

  edges:
    - from: 0 # AR â†’ Diffusion
      to: 1
      window_size: -1

# OmniConnector configuration for efficient inter-stage tensor transfer
connectors:
  - type: tensor_transfer
    source_stage: 0
    target_stage: 1
    # Transfer prior_token_ids efficiently between stages
    fields:
      - name: prior_token_ids
        dtype: int64
      - name: prior_token_image_ids
        dtype: int64
        optional: true
